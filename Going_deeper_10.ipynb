{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Going_deeper_10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e82690467a2c42acb9f52dc9b3fae25b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6e37ccb1b83d43859b02eaecbdb06daf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_874c1d61b8184effa04fa41931930d07",
              "IPY_MODEL_94f08ebca32d4acdbc786a456f563f14"
            ]
          }
        },
        "6e37ccb1b83d43859b02eaecbdb06daf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "874c1d61b8184effa04fa41931930d07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bd953b9314dc43689899af9626110fe9",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 78968,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 78968,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1bbac1c2b9674ffc92bf9db620b34765"
          }
        },
        "94f08ebca32d4acdbc786a456f563f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eef9db46be7d4e3bb1556439d9b77815",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 78968/78968 [02:07&lt;00:00, 620.49it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_60127ec511254c259a3ec5fc40fb5cc7"
          }
        },
        "bd953b9314dc43689899af9626110fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1bbac1c2b9674ffc92bf9db620b34765": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eef9db46be7d4e3bb1556439d9b77815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "60127ec511254c259a3ec5fc40fb5cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b436f39702b644e4a94b6f0be88ddb8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6adbc7b5c02c4544a352a6acdf78acd7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1a5dbb5770a04f5b994422d00cb64d3f",
              "IPY_MODEL_5beab19bebe54d05af2bf4260fbe0f4b"
            ]
          }
        },
        "6adbc7b5c02c4544a352a6acdf78acd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a5dbb5770a04f5b994422d00cb64d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_40232ff812a346f69f3b53a618c59be6",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1080,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ad560f3b2644d37a2d56d8ba8431557"
          }
        },
        "5beab19bebe54d05af2bf4260fbe0f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1394a0efa78d461fa47cf4290208acfa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/1080 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_028558f84b4245ebabbb854100aae059"
          }
        },
        "40232ff812a346f69f3b53a618c59be6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ad560f3b2644d37a2d56d8ba8431557": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1394a0efa78d461fa47cf4290208acfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "028558f84b4245ebabbb854100aae059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQgphzJZKXKf"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import random\n",
        "\n",
        "import seaborn # Attention 시각화를 위해 필요!"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5ys2o7qwAL9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pV14lHBU9xh",
        "outputId": "1b652554-f443-48bc-bd63-0c25e4687e2c"
      },
      "source": [
        "!wget https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-18 14:27:11--  https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-news-v1/korean-english-park.train.tar.gz [following]\n",
            "--2020-11-18 14:27:11--  https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-news-v1/korean-english-park.train.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8718893 (8.3M) [application/octet-stream]\n",
            "Saving to: ‘korean-english-park.train.tar.gz’\n",
            "\n",
            "korean-english-park 100%[===================>]   8.31M  53.8MB/s    in 0.2s    \n",
            "\n",
            "2020-11-18 14:27:12 (53.8 MB/s) - ‘korean-english-park.train.tar.gz’ saved [8718893/8718893]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxIaGwY3YgHe",
        "outputId": "096154f8-e8f3-4aba-8374-636f8863eb9f"
      },
      "source": [
        "!gzip -d korean-english-park.train.tar.gz\n",
        "!tar -xvf korean-english-park.train.tar"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "korean-english-park.train.en\n",
            "korean-english-park.train.ko\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehxHWicFZUKB",
        "outputId": "89d391ce-34b8-43e5-f907-b3e46a86ceb1"
      },
      "source": [
        "path_to_kr = '/content/korean-english-park.train.ko'\n",
        "path_to_en = '/content/korean-english-park.train.en'\n",
        "\n",
        "with open(path_to_kr, \"r\") as f:\n",
        "    ko = f.read().splitlines()\n",
        "\n",
        "with open(path_to_en, \"r\") as f:\n",
        "    en = f.read().splitlines()\n",
        "\n",
        "print(\"Data Size:\", len(ko))\n",
        "print(\"Korean Example:\")\n",
        "\n",
        "for sen in ko[0:100][::20]: print(\">>\", sen)\n",
        "\n",
        "print(\"Data Size:\", len(en))\n",
        "print(\"English Example:\")\n",
        "\n",
        "for sen in en[0:100][::20]: print(\">>\", sen)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Size: 94123\n",
            "Korean Example:\n",
            ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
            ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
            ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
            ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
            ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n",
            "Data Size: 94123\n",
            "English Example:\n",
            ">> Much of personal computing is about \"can you top this?\"\n",
            ">> Amid mounting pressure on North Korea to abandon its nuclear weapons program Japanese and North Korean diplomats have resumed talks on normalizing diplomatic relations.\n",
            ">> “Guard robots are used privately and professionally to detect intruders or fire,” Karlsson said.\n",
            ">> Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",
            ">> Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSsZUlQ0Z2ZB",
        "outputId": "b2305813-e629-4943-fc6f-8565739576a0"
      },
      "source": [
        "cleaned_corpus = list(set(zip(ko, en)))\n",
        "cleaned_corpus[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('하리리와 22명은 2005년 2월 14일 베이루트 중심가에서 자동차 퍼레이드 도중 폭탄 테러로 즉사했다.',\n",
              " 'Hariri and 22 others were killed in a huge explosion that occurred as his motorcade was passing through central Beirut.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "SKJYNzsydnts",
        "outputId": "e8e34603-f0ee-449f-91ad-ae3d8abb0da7"
      },
      "source": [
        "def preprocess_sentence_en(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z?.!,1-9\\\\s]\", \"\", sentence)\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "preprocess_sentence_en(cleaned_corpus[0][1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hariri and 22 others were killed in a huge explosion that occurred as his motorcade was passing through central beirut .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "qFQhnHOFwY2Y",
        "outputId": "9d0b91b2-9ad0-40a5-cc7c-e8521f0141dd"
      },
      "source": [
        "def preprocess_sentence_ko(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ?.!,1-9\\\\s]\", \"\", sentence)\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "preprocess_sentence_ko(cleaned_corpus[0][0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'하리리와 22명은 25년 2월 14일 베이루트 중심가에서 자동차 퍼레이드 도중 폭탄 테러로 즉사했다 .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04sfgaUwdpvN"
      },
      "source": [
        "kor_corpus = []\n",
        "eng_corpus = []\n",
        "for a in range(len(cleaned_corpus)):\n",
        "    eng_sentence = preprocess_sentence_en(cleaned_corpus[a][1]) # 번역문\n",
        "    kor_sentence = preprocess_sentence_ko(cleaned_corpus[a][0]) #번역해야할 문장\n",
        "    eng_corpus.append(eng_sentence)\n",
        "    kor_corpus.append(kor_sentence)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ZWkuQQsSlcYT",
        "outputId": "3060f2d5-6627-4bc3-bf37-4495434fc600"
      },
      "source": [
        "kor_corpus[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'하리리와 22명은 25년 2월 14일 베이루트 중심가에서 자동차 퍼레이드 도중 폭탄 테러로 즉사했다 .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9ysXdzIw0fx",
        "outputId": "0d9656d7-7190-4138-8701-f12e5b4cd818"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 12.0MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sNaWd3tdsQW"
      },
      "source": [
        "#Going-deeper-12\n",
        "import sentencepiece as spm\n",
        "\n",
        "def generate_ko_tokenizer(corpus,\n",
        "                       vocab_size,\n",
        "                       lang=\"ko\",\n",
        "                       pad_id=0,\n",
        "                       bos_id=1,\n",
        "                       eos_id=2,\n",
        "                       unk_id=3):\n",
        "    file = \"./%s_corpus.txt\" % lang\n",
        "    model = \"%s_spm\" % lang\n",
        "\n",
        "    with open(file, 'w') as f:\n",
        "        for row in corpus: f.write('{}\\n'.format(row))\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
        "        % (file, model, vocab_size) + \\\n",
        "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
        "        % (pad_id, bos_id, eos_id, unk_id)\n",
        "    )\n",
        "\n",
        "    tokenizer = spm.SentencePieceProcessor()\n",
        "    tokenizer.Load('%s.model' % model)\n",
        "\n",
        "    return tokenizer"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiscyuIpeMIb"
      },
      "source": [
        "def generate_eng_tokenizer(corpus,\n",
        "                       vocab_size,\n",
        "                       lang=\"eng\",\n",
        "                       pad_id=0,\n",
        "                       bos_id=1,\n",
        "                       eos_id=2,\n",
        "                       unk_id=3):\n",
        "    file = \"./%s_corpus.txt\" % lang\n",
        "    model = \"%s_spm\" % lang\n",
        "\n",
        "    with open(file, 'w') as f:\n",
        "        for row in corpus: f.write('{}\\n'.format(row))\n",
        "\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
        "        % (file, model, vocab_size) + \\\n",
        "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
        "        % (pad_id, bos_id, eos_id, unk_id)\n",
        "    )\n",
        "\n",
        "    tokenizer = spm.SentencePieceProcessor()\n",
        "    tokenizer.Load('%s.model' % model)\n",
        "\n",
        "    return tokenizer"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w1Eevl1eSmU",
        "outputId": "bbecdf7a-1d69-443c-dbba-5fa3afc9cee2"
      },
      "source": [
        "VOCAB_SIZE = 20000\n",
        "tokenizer_ko = generate_ko_tokenizer(kor_corpus, VOCAB_SIZE)\n",
        "tokenizer_eng = generate_eng_tokenizer(eng_corpus, VOCAB_SIZE)\n",
        "tokenizer_eng.set_encode_extra_options('bos:eos')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "e82690467a2c42acb9f52dc9b3fae25b",
            "6e37ccb1b83d43859b02eaecbdb06daf",
            "874c1d61b8184effa04fa41931930d07",
            "94f08ebca32d4acdbc786a456f563f14",
            "bd953b9314dc43689899af9626110fe9",
            "1bbac1c2b9674ffc92bf9db620b34765",
            "eef9db46be7d4e3bb1556439d9b77815",
            "60127ec511254c259a3ec5fc40fb5cc7"
          ]
        },
        "id": "LDmRw2r-k97D",
        "outputId": "0f8cef29-8149-40c4-a4c6-4ccf296c30d1"
      },
      "source": [
        "from tqdm.notebook import tqdm   # Process 과정을 보기 위해\n",
        "\n",
        "src_corpus = []\n",
        "tgt_corpus = []\n",
        "\n",
        "for a in tqdm(range(len(cleaned_corpus))):\n",
        "\n",
        "    src_tokens = tokenizer_ko.encode_as_ids(kor_corpus[a])\n",
        "    tgt_tokens = tokenizer_eng.encode_as_ids(eng_corpus[a])\n",
        "\n",
        "    if (len(src_tokens) > 50): continue\n",
        "    if (len(tgt_tokens) > 50): continue\n",
        "    \n",
        "    src_corpus.append(src_tokens)\n",
        "    tgt_corpus.append(tgt_tokens)\n",
        "\n",
        "len(src_corpus)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e82690467a2c42acb9f52dc9b3fae25b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=78968.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_b_txpl0Ik8",
        "outputId": "a99e9896-6ee9-4023-9f1e-d3201c4ae386"
      },
      "source": [
        "#data prepare\n",
        "from sklearn.model_selection import train_test_split\n",
        "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
        "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')\n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = \\\n",
        "train_test_split(enc_tensor, dec_tensor, test_size=0.01)\n",
        "\n",
        "print(len(enc_train), len(enc_val), len(dec_train), len(dec_val))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69086 698 69086 698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvosYQ3pxOoh"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi1cPTGOKYVn"
      },
      "source": [
        "# positional encoding\n",
        "\n",
        "def positional_encoding(pos, d_model):\n",
        "\n",
        "    # sin cos \n",
        "    def cal_angle(position, i):\n",
        "        return position / np.power(10000, int(i) / d_model)\n",
        "\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i) for i in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
        "\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "\n",
        "    return sinusoid_table"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCuTmhnDKrRh"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.depth = d_model // self.num_heads\n",
        "        \n",
        "        self.W_q = tf.keras.layers.Dense(d_model) # Linear Layer\n",
        "        self.W_k = tf.keras.layers.Dense(d_model)\n",
        "        self.W_v = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "        self.linear = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
        "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
        "\n",
        "        \"\"\"\n",
        "        Scaled QK 값 구하기\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        1. Attention Weights 값 구하기 -> attentions\n",
        "        2. Attention 값을 V에 곱하기 -> out\n",
        "        \"\"\" \n",
        "        Q_Kt = tf.matmul(Q,K, transpose_b=True)\n",
        "\n",
        "        scaled_qk =Q_kt/tf.math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None: scaled_qk += (mask * -1e9) \n",
        "\n",
        "        return out, attentions\n",
        "        \n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"\n",
        "        Embedding을 Head의 수로 분할하는 함수\n",
        "\n",
        "        x: [ batch x length x emb ]\n",
        "        return: [ batch x length x heads x self.depth ]\n",
        "        \"\"\"\n",
        "\n",
        "        #batchsize\n",
        "        bsz = x.shape[0]\n",
        "        split_x = tf.reshape(x,(bsz,-1,self.num_heads,self.depth))\n",
        "        split_x = tf.transpose(split_X, perm = [0,2,1,3])\n",
        "\n",
        "        return split_x\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        \"\"\"\n",
        "        분할된 Embedding을 하나로 결합하는 함수\n",
        "\n",
        "        x: [ batch x length x heads x self.depth ]\n",
        "        return: [ batch x length x emb ]\n",
        "        \"\"\"\n",
        "        bsz = x.shape[0]\n",
        "        combined_x = tf.transpose(x, perm[0,2,1,3])\n",
        "        combined_x = tf.reshape(combined_x,(bsz,-1,self.d_model))\n",
        "\n",
        "        return combined_x\n",
        "    \n",
        "\n",
        "    def call(self, Q, K, V, mask):\n",
        "        \"\"\"\n",
        "        아래 순서에 따라 소스를 작성하세요.\n",
        "\n",
        "        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
        "        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
        "        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
        "                 -> out, attention_weights\n",
        "        Step 4: Combine Heads(out) -> out\n",
        "        Step 5: Linear_out(out) -> out\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        WQ = self.W_q(Q)\n",
        "        WK = self.W_k(K)\n",
        "        WV = self.W_v(V)\n",
        "\n",
        "        WQ_splits = self.split_heads(WQ)\n",
        "        WK_splits = self.split_heads(WK)\n",
        "        WV_splits = self.split_heads(WV)\n",
        "\n",
        "        out, attention_weights = self.scaled_dot_product_attention(WQ_splits,WK_splits,\n",
        "                                                                   WV_splits,mask)\n",
        "        \n",
        "        out = self.combine_heads(out)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out, attention_weights"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClNykryzMoQh"
      },
      "source": [
        "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        # dff = 2048(paper), d_model = 512(paper)\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu') # 2048\n",
        "        self.w_2 = tf.keras.layers.Dense(d_model) # 512\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.w_1(x)\n",
        "        out = self.w_2(out)\n",
        "            \n",
        "        return out"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUTQ4YJ0NA49"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "        \n",
        "    def call(self, x, mask):\n",
        "\n",
        "        \"\"\"\n",
        "        Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "        \n",
        "        \"\"\"\n",
        "        Position-Wise Feed Forward Network\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "        \n",
        "        return out, enc_attn"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgNt5t6HNE5W"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer,self).__init__()\n",
        "\n",
        "        self.dec_self_attn = MultiHeadAttention(d_model,num_heads)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model,num_heads)\n",
        "\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model,d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon= 1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon= 1e-6)\n",
        "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon= 1e-6)\n",
        "\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "    def call(self, x, enc_out, casuality_mask, padding_mask):\n",
        "\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, dec_attn = self.dec_self_attn(out,out,out,casuality_mask)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out, dec_attn = self.dec_enc_attn(out,enc_out,enc_out,padding_mask)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        residual = out\n",
        "        out = self.norm_3(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        return out, dec_attn, dec_enc_attn\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czmERudWPa3q"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
        "                        for _ in range(n_layers)]\n",
        "\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        out = x\n",
        "        enc_attns = list()\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            out, enc_attn = self.enc_layers[i](out,mask)\n",
        "            enc_attns.append(enc_attn)\n",
        "\n",
        "        return out, enc_attns"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlLVm6ndQwRj"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 d_ff,\n",
        "                 dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
        "                            for _ in range(n_layers)]\n",
        "                            \n",
        "                            \n",
        "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "        out = x\n",
        "    \n",
        "        dec_attns = list()\n",
        "        dec_enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, dec_attn, dec_enc_attn = \\\n",
        "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
        "\n",
        "            dec_attns.append(dec_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "\n",
        "        return out, dec_attns, dec_enc_attns"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aphQ9qu0Q1VJ"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 d_ff,\n",
        "                 src_vocab_size,\n",
        "                 tgt_vocab_size,\n",
        "                 pos_len,\n",
        "                 dropout=0.2,\n",
        "                 shared_fc=True,\n",
        "                 shared_emb=True):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "\n",
        "        \"\"\"\n",
        "        1. Embedding Layer 정의\n",
        "        2. Positional Encoding 정의\n",
        "        3. Encoder / Decoder 정의\n",
        "        4. Output Linear 정의\n",
        "        5. Shared Weights\n",
        "        6. Dropout 정의\n",
        "        \"\"\"\n",
        "\n",
        "        if shared_emb:\n",
        "            self.enc_emb = self.dec_emb = \\\n",
        "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "        else:\n",
        "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
        "\n",
        "        self.shared_fc = shared_fc\n",
        "\n",
        "        if shared_fc:\n",
        "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
        "\n",
        "    def embedding(self, emb, x):\n",
        "        \"\"\"\n",
        "        입력된 정수 배열을 Embedding + Pos Encoding\n",
        "        + Shared일 경우 Scaling 작업 포함\n",
        "\n",
        "        x: [ batch x length ]\n",
        "        return: [ batch x length x emb ]\n",
        "        \"\"\"\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        out = emb(x)\n",
        "\n",
        "        if self.shared_fc: \n",
        "            out *= tf.math.sqrt(self.d_model)\n",
        "            out += self.pos_encoding[np.newaxis, ...][:,:seq_len,:]\n",
        "            out = self.do(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "        \n",
        "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
        "        \"\"\"\n",
        "        아래 순서에 따라 소스를 작성하세요.\n",
        "\n",
        "        Step 1: Embedding(enc_in, dec_in) -> enc_in, dec_in\n",
        "        Step 2: Encoder(enc_in, enc_mask) -> enc_out, enc_attns\n",
        "        Step 3: Decoder(dec_in, enc_out, mask)\n",
        "                -> dec_out, dec_attns, dec_enc_attns\n",
        "        Step 4: Out Linear(dec_out) -> logits\n",
        "        \"\"\"\n",
        "\n",
        "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
        "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
        "\n",
        "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
        "\n",
        "        dec_out, dec_attns, dec_enc_attns = \\\n",
        "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
        "\n",
        "        logits = self.fc(dec_out)\n",
        "        \n",
        "        return logits, enc_attns, dec_attns, dec_enc_attns"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmD5NW31xRnm"
      },
      "source": [
        "n_layers= 2\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "d_ff    = 2048\n",
        "dropout = 0.3 #dropout_rate\n",
        "\n",
        "transformer = Transformer(\n",
        "    n_layers=n_layers,\n",
        "    d_model=d_model,\n",
        "    n_heads=n_heads,\n",
        "    d_ff=d_ff,\n",
        "    src_vocab_size=VOCAB_SIZE,\n",
        "    tgt_vocab_size=VOCAB_SIZE,\n",
        "    pos_len=200,\n",
        "    dropout=dropout,\n",
        "    shared_fc=True,\n",
        "    shared_emb=True)\n",
        "\n",
        "d_model = 512"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE44UgrBUwHu"
      },
      "source": [
        "\n",
        "def generate_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def generate_causality_mask(src_len, tgt_len):\n",
        "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
        "    return tf.cast(mask, tf.float32)\n",
        "\n",
        "def generate_masks(src, tgt):\n",
        "    enc_mask = generate_padding_mask(src)\n",
        "    dec_mask = generate_padding_mask(tgt)\n",
        "\n",
        "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
        "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
        "\n",
        "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
        "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
        "\n",
        "    return enc_mask, dec_enc_mask, dec_mask"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "-bJh1M8oUxgk",
        "outputId": "5753acf1-0d0e-4f99-b9f5-1d1217b2b0cf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "batch, length = 16, 20\n",
        "src_padding = 5\n",
        "tgt_padding = 15\n",
        "\n",
        "src_pad = tf.zeros(shape=(batch, src_padding))\n",
        "tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
        "\n",
        "sample_data = tf.ones(shape=(batch, length))\n",
        "\n",
        "sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
        "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
        "\n",
        "enc_mask, dec_enc_mask, dec_mask = \\\n",
        "generate_masks(sample_src, sample_tgt)\n",
        "\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "\n",
        "ax1 = fig.add_subplot(131)\n",
        "ax2 = fig.add_subplot(132)\n",
        "ax3 = fig.add_subplot(133)\n",
        "\n",
        "ax1.set_title('1) Encoder Mask')\n",
        "ax2.set_title('2) Encoder-Decoder Mask')\n",
        "ax3.set_title('3) Decoder Mask')\n",
        "\n",
        "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
        "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
        "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7JqM-gOU1Ug"
      },
      "source": [
        "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(LearningRateScheduler, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = step ** -0.5\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        \n",
        "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = LearningRateScheduler(512)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRW8CtckxfLR"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riKEwm9NznpB"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ2wU5lozlCW"
      },
      "source": [
        "@tf.function()\n",
        "def train_step(src, tgt, model, optimizer):\n",
        "    tgt_in = tgt[:, :-1]\n",
        "    gold = tgt[:, 1:]\n",
        "\n",
        "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
        "        loss = loss_function(gold, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, enc_attns, dec_attns, dec_enc_attns"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9Z6CDR0zr6o"
      },
      "source": [
        "### Attention Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFq0lKptzraK"
      },
      "source": [
        "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
        "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
        "        import seaborn\n",
        "        seaborn.heatmap(data, \n",
        "                        square=True,\n",
        "                        vmin=0.0, vmax=1.0, \n",
        "                        cbar=False, ax=ax,\n",
        "                        xticklabels=x,\n",
        "                        yticklabels=y)\n",
        "\n",
        "    for layer in range(0, 2, 1):\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
        "        print(\"Encoder Layer\", layer + 1)\n",
        "        for h in range(4):\n",
        "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
        "        plt.show()\n",
        "\n",
        "    for layer in range(0, 2, 1):\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
        "        print(\"Decoder Self Layer\", layer+1)\n",
        "        for h in range(4):\n",
        "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Decoder Src Layer\", layer+1)\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
        "        for h in range(4):\n",
        "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
        "        plt.show()"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l01YqJ4TzydC"
      },
      "source": [
        "### 번역 생성 및 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--ClzCdsznDg"
      },
      "source": [
        "\n",
        "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
        "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
        "\n",
        "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
        "                                                           maxlen=enc_train.shape[-1],\n",
        "                                                           padding='post')\n",
        "\n",
        "    ids = []\n",
        "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
        "    for i in range(dec_train.shape[-1]):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
        "        generate_masks(_input, output)\n",
        "\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
        "        model(_input, \n",
        "              output,\n",
        "              enc_padding_mask,\n",
        "              combined_mask,\n",
        "              dec_padding_mask)\n",
        "\n",
        "        predicted_id = \\\n",
        "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
        "\n",
        "        if tgt_tokenizer.eos_id() == predicted_id:\n",
        "            result = tgt_tokenizer.decode_ids(ids)\n",
        "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
        "\n",
        "        ids.append(predicted_id)\n",
        "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
        "\n",
        "    result = tgt_tokenizer.decode_ids(ids)\n",
        "\n",
        "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSRl55-0z2a4"
      },
      "source": [
        "## 번역 생성 및 Attention 시각화 결합"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3ZWnvbhz0yQ"
      },
      "source": [
        "\n",
        "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
        "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    if plot_attention:\n",
        "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDVzxEjyz3t7"
      },
      "source": [
        "sentences = ['오바마는 대통령이다.', '시민들은 도시 속에 산다.', '커피는 필요 없다.', '일곱 명의 사망자가 발생했다.']"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481,
          "referenced_widgets": [
            "b436f39702b644e4a94b6f0be88ddb8c",
            "6adbc7b5c02c4544a352a6acdf78acd7",
            "1a5dbb5770a04f5b994422d00cb64d3f",
            "5beab19bebe54d05af2bf4260fbe0f4b",
            "40232ff812a346f69f3b53a618c59be6",
            "7ad560f3b2644d37a2d56d8ba8431557",
            "1394a0efa78d461fa47cf4290208acfa",
            "028558f84b4245ebabbb854100aae059"
          ]
        },
        "id": "Pf4rYDolz7aH",
        "outputId": "72b6c892-49b2-40b6-c321-92936a152147"
      },
      "source": [
        "\n",
        "from tqdm import tqdm_notebook \n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 15\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "\n",
        "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
        "    random.shuffle(idx_list)\n",
        "    t = tqdm_notebook(idx_list)\n",
        "\n",
        "    for (batch, idx) in enumerate(t):\n",
        "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
        "                    dec_train[idx:idx+BATCH_SIZE],\n",
        "                    transformer,\n",
        "                    optimizer)\n",
        "\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
        "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
        "    \n",
        "    print('Translations')\n",
        "    for i in sentences:\n",
        "        translate(sentence, transformer, tokenizer_ko, tokenizer_en, plot_attention=False)\n",
        "    print('\\n','Hyperparameters')\n",
        "    print('> n_layers:', N_LAYERS)\n",
        "    print('> d_model:', D_MODEL)\n",
        "    print('> n_heads:', N_HEADS)\n",
        "    print('> d_ff:', D_FF)\n",
        "    print('> dropout:', DROPOUT)\n",
        "    print('\\n','Training Parameters')\n",
        "    print('> Warmup Steps: 4000')\n",
        "    print('> Batch Size: 64')\n",
        "    print('> Epoch At', epoct+1)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b436f39702b644e4a94b6f0be88ddb8c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1080.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-a25249232895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mdec_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                     optimizer)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYPAye2oz9G7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}